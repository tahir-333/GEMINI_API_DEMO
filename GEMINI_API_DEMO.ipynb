{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgyFzeRT1j4nIZQidFpQbT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahir-333/ASSIGNMENT_SOLUTONS/blob/main/GEMINI_API_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gemini API Demo**\n",
        "* Google Gemini is an advanced tool that we can leverage to interact, ask questions, and receive intelligent responses on the gemini platform. While it offers extensive functionality, it doesn't allow direct integration of Google Gemini into our code.\n",
        "\n",
        "* In this demo, we'll explore how to utilize the gemini API to bring the capabilities of a large language model (LLM) into our own applications. This approach allows us to harness the power of Google Gemini's technology programmatically, making it a flexible and valuable addition to our projects.\n",
        "\n",
        "# **Installing the Gemini SDK**\n",
        "* To use the Gemini API in Python, we will use the official Gemini SDK for Python. This SDK simplifies the process of interacting with the API in your Python applications."
      ],
      "metadata": {
        "id": "f_t_3xD9yTAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install or upgrade the Gemini Python library.\n",
        "# The `--upgrade` flag ensures you have the latest version.\n",
        "# The `--quiet` flag suppresses unnecessary output during installation.\n",
        "# The `!` (or `%` for line magics) is used in Colab or Jupyter notebooks to execute shell commands.\n",
        "# In this case, it runs the `pip` command to manage Python packages.\n",
        "\n",
        "!pip install --upgrade --quiet google generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BukIH8nWffXn",
        "outputId": "68e726dd-b297-4350-a757-96ddc53acafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/45.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get and Setup Your API Key**\n",
        "* To use the Gemini API, you'll need an API key. If you don't already have one, create a key in Google AI Studio.\n",
        "\n",
        "* In Colab, add the key to the secrets manager under the \"üîë\" in the left panel."
      ],
      "metadata": {
        "id": "aDXFeIeozH91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the `userdata` module from `google.colab` to access user-specific data.\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Get the Gemini API key from user data and store it in the `GEMINI_API_KEY` variable.\n",
        "# The key is expected to be stored under the key 'GEMINI_API_KEY'.\n",
        "# We use type hinting to specify that the variable should hold a string value.\n",
        "GEMINI_API_KEY : str = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Check if the API key was successfully retrieved.\n",
        "if GEMINI_API_KEY :\n",
        "  print(\"The Gemni API key fetched fuccessfully.\")\n",
        "\n",
        "# If the key is not found, print an error message and instructions for the user.\n",
        "else:\n",
        "  print(\"The Gemni API key not fetched.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5AiQgXXfw9X",
        "outputId": "e5e2c5e1-c8df-4bf4-ce55-56edec41f1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Gemni API key fetched fuccessfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configure the Gemini SDK**\n",
        "* Now that we have installed the Gemini SDK and obtained an API key, the next step is to configure the SDK to use the key. This setup ensures that our code can authenticate with gemini's servers and make API requests.\n",
        "\n",
        "# **Why Configure?**\n",
        "**Configuring the SDK with your API key:**\n",
        "\n",
        "1. Authenticates your application with gemini's services.\n",
        "2. Allows you to send requests and receive responses securely.\n",
        "3. Simplifies API usage in your Python scripts or notebooks."
      ],
      "metadata": {
        "id": "m5Dbl-8SzqKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the genai module for API interaction\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure it with your api key\n",
        "genai.configure(api_key = GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "l9cmNcA_h85D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check all available methods in the genai module\n",
        "dir(genai)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA2d351WiQYW",
        "outputId": "f1a71c92-befb-45e0-8e49-42d201f73c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ChatSession',\n",
              " 'GenerationConfig',\n",
              " 'GenerativeModel',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " 'annotations',\n",
              " 'caching',\n",
              " 'configure',\n",
              " 'create_tuned_model',\n",
              " 'delete_file',\n",
              " 'delete_tuned_model',\n",
              " 'embed_content',\n",
              " 'embed_content_async',\n",
              " 'get_base_model',\n",
              " 'get_file',\n",
              " 'get_model',\n",
              " 'get_operation',\n",
              " 'get_tuned_model',\n",
              " 'list_files',\n",
              " 'list_models',\n",
              " 'list_operations',\n",
              " 'list_tuned_models',\n",
              " 'protos',\n",
              " 'responder',\n",
              " 'string_utils',\n",
              " 'types',\n",
              " 'update_tuned_model',\n",
              " 'upload_file',\n",
              " 'utils']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Select an Configure Gemini Model**\n",
        "* Gemini offers several models with varying capabilities and pricing. Selecting the right model depends on your use case and budget. In this section, we'll discuss model selection.\n",
        "\n",
        "* To minimize costs while still achieving good results, we'll use gemini-1.5-flash, which offers excellent performance at a lower price."
      ],
      "metadata": {
        "id": "TesM3VnW0MyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get all available models from gemini\n",
        "for model in genai.list_models():\n",
        "  print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ASG1MycKiak4",
        "outputId": "06a342b4-6863-400f-9cac-20ffbecb91bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 Chat (Legacy)',\n",
            "      description='A legacy text-only model optimized for chat conversations',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 (Legacy)',\n",
            "      description='A legacy model that understands text and generates text as an output',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Latest',\n",
            "      description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
            "                   'February 15th, 2025. Move to a newer Gemini version.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
            "      description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
            "                   'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
            "                   'version.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-1.5-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
            "                   'million tokens.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro 001',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Pro 002',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro-exp-0801',\n",
            "      base_model_id='',\n",
            "      version='exp-0801',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='exp-1206',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
            "                   'across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-001-tuning',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
            "      description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
            "                   'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=16384,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
            "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='exp-1206',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Flash 002',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
            "                   'released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
            "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
            "      description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Experimental',\n",
            "      description='Gemini 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-exp-1206',\n",
            "      base_model_id='',\n",
            "      version='exp_1206',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1121',\n",
            "      base_model_id='',\n",
            "      version='exp-1206',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1114',\n",
            "      base_model_id='',\n",
            "      version='exp-1206',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
            "      base_model_id='',\n",
            "      version='2.0-exp-01-21',\n",
            "      display_name='Gemini 2.0 Flash Thinking Experimental 01-21',\n",
            "      description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0-exp-01-21',\n",
            "      display_name='Gemini 2.0 Flash Thinking Experimental 01-21',\n",
            "      description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Thinking Experimental',\n",
            "      description='Gemini 2.0 Flash Thinking Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/learnlm-1.5-pro-experimental',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='LearnLM 1.5 Pro Experimental',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
            "                   'mid-size multimodal model that supports up to 2 million tokens.'),\n",
            "      input_token_limit=32767,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import the `GenerativeModel` class from the Google Generative AI library.\n",
        "# This class is used to define and interact with specific generative models\n",
        "from google.generativeai.generative_models import GenerativeModel\n",
        "\n",
        "# Instantiate a generative model object using the \"gemini-1.5-flash\" model.\n",
        "# This model represents a specific version of Google's Gemini language model,\n",
        "# optimized for speed and performance in generating responses.\n",
        "model : GenerativeModel = genai.GenerativeModel(\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "Yq41r2cdpr7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run your first prompt**\n",
        "\n",
        "# **Make a Request**\n",
        "* Use the generate_content method to generate responses to your prompts. You can pass text directly to generate_content, and use the .text property to get the text content of the response."
      ],
      "metadata": {
        "id": "EYtZ1oTo0uJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# list all available methods for the model\n",
        "dir(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zhqcYo8iiaZ",
        "outputId": "8121a47f-be85-456d-8596-0a7bf61c970b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_async_client',\n",
              " '_client',\n",
              " '_generation_config',\n",
              " '_get_tools_lib',\n",
              " '_model_name',\n",
              " '_prepare_request',\n",
              " '_safety_settings',\n",
              " '_system_instruction',\n",
              " '_tool_config',\n",
              " '_tools',\n",
              " 'cached_content',\n",
              " 'count_tokens',\n",
              " 'count_tokens_async',\n",
              " 'from_cached_content',\n",
              " 'generate_content',\n",
              " 'generate_content_async',\n",
              " 'model_name',\n",
              " 'start_chat']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import the `GenerateContentResponse` class from Google's Generative AI library.\n",
        "# This class is used to type hint the response object returned by the `generate_content` method.\n",
        "from google.generativeai.types.generation_types import GenerateContentResponse\n",
        "\n",
        "# Generate content using the generative model.\n",
        "# The `generate_content` method sends a prompt to the model and returns a structured response.\n",
        "response : GenerateContentResponse = model.generate_content( # Changed generatecontent to generate_content\n",
        "    \"Explain the basics of Machine Learning in simple terms. Respond in the Markdown formate\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Q7KXVH-yqeDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Displaying the Response**\n",
        "* You can use the Markdown library to display the response in a well-structured format, ensuring that the content is easily readable and properly formatted. This is particularly useful when presenting information that includes headings, lists, code blocks, links, and other Markdown features."
      ],
      "metadata": {
        "id": "LwMSOGGQ1MTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import the Markdown class from the IPython.display module\n",
        "# This is used to render Markdown content in the notebook output\n",
        "from IPython.display import Markdown\n"
      ],
      "metadata": {
        "id": "EP5F7WUmtMQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "M_X_oYk8teJi",
        "outputId": "eeb1ebcf-d4b4-436d-f9dd-768854b41270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Machine Learning Basics: Explained Simply\n\nMachine learning is a type of artificial intelligence (AI) that allows computers to learn from data without being explicitly programmed.  Instead of giving the computer a set of rules to follow, we give it data and let it figure out the rules itself.  Think of it like teaching a dog a trick:  instead of telling it exactly how to sit, you show it what \"sit\" looks like and reward it when it gets close.\n\nHere's a breakdown of the key concepts:\n\n**1. Data is King:**  Machine learning algorithms need lots of data to learn from. This data can be anything ‚Äì images, text, numbers, sensor readings, etc.  The more data you have, the better the algorithm usually performs.\n\n**2. Algorithms are the Learners:** These are the mathematical instructions that tell the computer how to learn from the data. Different algorithms are suited for different types of tasks. Some common types include:\n\n* **Supervised Learning:** The algorithm is trained on a dataset with labeled examples (e.g., images of cats labeled \"cat,\" images of dogs labeled \"dog\").  It learns to predict the label for new, unseen data.  Think of it like showing a child pictures and telling them the name of each animal.\n\n* **Unsupervised Learning:** The algorithm is trained on a dataset without labels.  It tries to find patterns and structures in the data on its own.  Think of it like giving a child a box of toys and letting them sort them into groups based on their own observations.\n\n* **Reinforcement Learning:** The algorithm learns through trial and error by interacting with an environment. It receives rewards for good actions and penalties for bad actions.  Think of it like training a dog with treats and corrections.\n\n\n**3. Training and Prediction:**\n\n* **Training:** This is the process of feeding the algorithm data so it can learn the underlying patterns.  This is like showing the dog the \"sit\" command repeatedly.\n\n* **Prediction:** Once the algorithm is trained, it can make predictions on new, unseen data. This is like testing the dog's ability to sit on command.\n\n\n**4. Evaluation:** After training, the algorithm's performance is evaluated to see how well it learned.  Metrics like accuracy, precision, and recall are used to measure this.\n\n\n**Simple Analogy:** Imagine you want to build a program to identify spam emails.  Instead of writing rules like \"if email contains 'free money' then mark as spam,\" you'd feed a machine learning algorithm a massive dataset of emails labeled as spam or not spam. The algorithm would learn the patterns associated with spam (certain words, sender addresses, etc.) and then use this knowledge to identify new spam emails.\n\n\nMachine learning is used in many applications today, including:\n\n* **Image recognition:** Identifying objects in images.\n* **Speech recognition:** Converting spoken words into text.\n* **Recommendation systems:** Suggesting products or movies you might like.\n* **Medical diagnosis:** Assisting doctors in diagnosing diseases.\n* **Self-driving cars:** Enabling cars to navigate roads autonomously.\n\n\nThis is a simplified explanation, but it covers the fundamental concepts of machine learning.  Each of these points can be expanded upon significantly, but this should give you a solid starting point.\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}